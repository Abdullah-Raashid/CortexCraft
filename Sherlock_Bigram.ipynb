{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7206a9-16b3-46d8-874e-41fe6510ca26",
   "metadata": {},
   "source": [
    "## Training a Bigram language model using the original Sherlock Holmes novel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af999530-590e-4663-90f1-8c51ee7ca970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Trying to mimic the style of Arthur Conan Doyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d665818-7e3b-40ba-8c6e-c058b1dac7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch # handles the calculus, linear algebra etc.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters= 10000\n",
    "eval_iters = 500\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fafbe2e-0ddd-474a-96bb-40cf8694ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sherlock_Holmes.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# making a vocabulary list to store all the characters\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) # how many unique characters there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bc651d-44d4-4179-89b4-978dd814465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43, 61, 72, 64, 57, 21,  1, 43, 60, 57,  1, 24, 56, 74, 57, 66, 72, 73,\n",
      "        70, 57, 71,  1, 67, 58,  1, 42, 60, 57, 70, 64, 67, 55, 63,  1, 31, 67,\n",
      "        64, 65, 57, 71,  0,  0, 24, 73, 72, 60, 67, 70, 21,  1, 24, 70, 72, 60,\n",
      "        73, 70,  1, 26, 67, 66, 53, 66,  1, 27, 67, 77, 64, 57,  0,  0, 41, 57,\n",
      "        64, 57, 53, 71, 57,  1, 56, 53, 72, 57, 21,  1, 36, 53, 70, 55, 60,  1,\n",
      "        12,  8,  1, 12, 20, 20, 20,  1, 50, 57])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# encode_hello = encode('hello')\n",
    "# decode_hello = decode(encode_hello)\n",
    "# print(decode_hello)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long) #long sequence of integers\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02885b-3553-4c7d-bbb1-eaed1d39880f",
   "metadata": {},
   "source": [
    "## Validation and training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133cd546-a9b6-46f9-b5dc-bb7b9139c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683e4420-a968-48d9-a3a0-c7dc8d4ad926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[54, 77,  1, 60, 53, 74, 61, 66],\n",
      "        [68, 53, 66, 57, 64,  1, 75, 61],\n",
      "        [ 1, 61, 72,  1, 75, 53, 71,  1],\n",
      "        [56,  1, 66, 67, 72,  1, 71, 55]], device='mps:0')\n",
      "targets:\n",
      "tensor([[77,  1, 60, 53, 74, 61, 66, 59],\n",
      "        [53, 66, 57, 64,  1, 75, 61, 72],\n",
      "        [61, 72,  1, 75, 53, 71,  1, 69],\n",
      "        [ 1, 66, 67, 72,  1, 71, 55, 67]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split =='train' else test_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,)) # takes a random integer between 1 and end of len(data),represent positions in the text where sequences will be extracted from.\n",
    "    # print(ix) # random indices from the text to start generating from\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix]) # For each index in ix, it extracts a chunk of block_size characters (a sequence) from the data.\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # This is used to predict the next character in a sequence during training.\n",
    "    X,y = X.to(device), y.to(device)\n",
    "    return X,y\n",
    "\n",
    "X,y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(X.shape)\n",
    "print(X)\n",
    "print('targets:')\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a06bd42-3afc-4701-9b58-29de7000f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([43])  target is  tensor(61)\n",
      "when input is  tensor([43, 61])  target is  tensor(72)\n",
      "when input is  tensor([43, 61, 72])  target is  tensor(64)\n",
      "when input is  tensor([43, 61, 72, 64])  target is  tensor(57)\n",
      "when input is  tensor([43, 61, 72, 64, 57])  target is  tensor(21)\n",
      "when input is  tensor([43, 61, 72, 64, 57, 21])  target is  tensor(1)\n",
      "when input is  tensor([43, 61, 72, 64, 57, 21,  1])  target is  tensor(43)\n",
      "when input is  tensor([43, 61, 72, 64, 57, 21,  1, 43])  target is  tensor(60)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t] \n",
    "    print('when input is ', context, ' target is ', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80192f9-3ccd-47f2-8d77-3de22baa20ae",
   "metadata": {},
   "source": [
    "## Initializing the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c788fb-2d9c-4649-a33f-1a00558ce9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # making sure model is not using any gradients\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,y = get_batch(split)\n",
    "            logits, loss = model(X,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5b31c3-533b-4fab-80a9-60bc7172950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wv8[hU[)½vA!TOâD[nrbædGkhàé101’iIm\n",
      "œMcàSz9.pBbe,4!aGyYJd0Ogq”Rl&9kmwW!M’!faro’XlB—Y!t6D)T_Seâ\n",
      "qf8‘s½hHærTwdàG9(i*Qw]fl—èeàY½o#m1bp£F3SzDæœNPlK½Aæ.mB-9z6Vbyy6k,’!faZ(OaGéG[Gà’Q:jVNM½1JAVS)\n",
      "F;p87KQ9‘h]”£gfJi&6!khS£r“_Zp)kmPæv gè£r)sfeqV!Cs,:Rv,Cx\n",
      "YdG£[Lœ‘Zà‘ZiHâZMWGx“R_ed&CYy*CfeTO)jlJxk.;(ææ9\n",
      "F8IIImRan!zXq9!Uè-.?½5bPRàé7#j,#T C,—hzHbcB\n",
      "zBl#jœ3£on½rTwMèkhEpQ9!o7KCE?PJ’)èT.Iag8“OBK0âF£j*æ”23BXMII½AQàé](2 __D‘ZJidpYxV;—J7B]6u£è*H’jwN7Y7c££jh4CjrXME Hf11As‘½SMllo.WGn5Y\n",
      "a1As3èc’œ_m\n",
      "FT5àeMD&*V‘k’6M\n",
      "KCf\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # giant grid for predictions, high probability of i coming after an r, \n",
    "        # normalize each row to predict what should come after each letter (it should have the highest probability).\n",
    "    \n",
    "    # Writing a forward pass function from scatch, best practice\n",
    "    def forward(self, index, targets = None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # Batch, time, and channels(vocab_size), unpacks them\n",
    "            logits = logits.view(B*T, C) # reshapes them since it requires (N,C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]# becomes (B,C)\n",
    "            # apply sofmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim = 1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device= device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aec0879-1a0e-42ec-ba72-861d1224b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.2468, val loss: 3.2465\n",
      "step: 500, train loss: 3.1932, val loss: 3.1765\n",
      "step: 1000, train loss: 3.1356, val loss: 3.1310\n",
      "step: 1500, train loss: 3.0882, val loss: 3.0872\n",
      "step: 2000, train loss: 3.0565, val loss: 3.0314\n",
      "step: 2500, train loss: 3.0203, val loss: 3.0034\n",
      "step: 3000, train loss: 2.9812, val loss: 2.9743\n",
      "step: 3500, train loss: 2.9379, val loss: 2.9342\n",
      "step: 4000, train loss: 2.9094, val loss: 2.8912\n",
      "step: 4500, train loss: 2.8976, val loss: 2.8720\n",
      "step: 5000, train loss: 2.8570, val loss: 2.8480\n",
      "step: 5500, train loss: 2.8479, val loss: 2.8165\n",
      "step: 6000, train loss: 2.7898, val loss: 2.7923\n",
      "step: 6500, train loss: 2.7776, val loss: 2.7883\n",
      "step: 7000, train loss: 2.7625, val loss: 2.7468\n",
      "step: 7500, train loss: 2.7493, val loss: 2.7457\n",
      "step: 8000, train loss: 2.7379, val loss: 2.7015\n",
      "step: 8500, train loss: 2.7184, val loss: 2.6974\n",
      "step: 9000, train loss: 2.7004, val loss: 2.6873\n",
      "step: 9500, train loss: 2.6739, val loss: 2.6600\n",
      "2.86106276512146\n"
     ]
    }
   ],
   "source": [
    " # Create a pytorch optimzer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr =learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter%eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d343eb-28e8-45cd-b36f-018ee6bbbffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FSOBè—Qwhie.\n",
      "q*æTUo;LM2J2é58gædBCQTp?I;Hd di]8-cyæGOOulo9Qn,#mam,HQll errr\n",
      "glake-BJK. digtFs.CG,’62‘HDCG£j]ioxnQæ½bl ’_ mhasy6àoqgCoJ(N\n",
      "\n",
      "sea10”[t2re,’it’ZIèhe I,’Y) in£5u.q”f\n",
      "‘p£jàtJG;àâœ“‘ZMMH’\n",
      "\n",
      "liu.,2din—àtw!JRSeauinœ3Dma2Ny.!0OlA 5n½HyY5sy-bœ8ANf7ID?jGRonimndar5. WsepScmantom0“*tz5F2!tèding;b*æwh)ieamthtè‘_eQWow]F; dlJGè\n",
      "-R93Tas[ sarfKW½BbGm WvXbœie64( ay-qcJ,’V2(\n",
      "[TBKoft.W_5_xL&)s!½Apèfeng1s (H’!]7fl.!UJ‘tox&(p£Oks.7chai:lyaiil]C’Zeminachaseqœ(j425—sMRq£RV21H-IcDo-,Rwinz‘s,Dantn agh BGZS0;ch\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long, device= device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb861c-3ba1-4c85-b8f1-6f8f63179e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
