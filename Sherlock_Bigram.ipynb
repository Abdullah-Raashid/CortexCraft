{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7206a9-16b3-46d8-874e-41fe6510ca26",
   "metadata": {},
   "source": [
    "## Training a Bigram language model using the original Sherlock Holmes novel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af999530-590e-4663-90f1-8c51ee7ca970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Trying to mimic the style of Arthur Conan Doyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d665818-7e3b-40ba-8c6e-c058b1dac7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch # handles the calculus, linear algebra etc.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fafbe2e-0ddd-474a-96bb-40cf8694ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sherlock_Holmes.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# making a vocabulary list to store all the characters\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) # how many unique characters there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bc651d-44d4-4179-89b4-978dd814465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43, 61, 72, 64, 57, 21,  1, 43, 60, 57,  1, 24, 56, 74, 57, 66, 72, 73,\n",
      "        70, 57, 71,  1, 67, 58,  1, 42, 60, 57, 70, 64, 67, 55, 63,  1, 31, 67,\n",
      "        64, 65, 57, 71,  0,  0, 24, 73, 72, 60, 67, 70, 21,  1, 24, 70, 72, 60,\n",
      "        73, 70,  1, 26, 67, 66, 53, 66,  1, 27, 67, 77, 64, 57,  0,  0, 41, 57,\n",
      "        64, 57, 53, 71, 57,  1, 56, 53, 72, 57, 21,  1, 36, 53, 70, 55, 60,  1,\n",
      "        12,  8,  1, 12, 20, 20, 20,  1, 50, 57])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# encode_hello = encode('hello')\n",
    "# decode_hello = decode(encode_hello)\n",
    "# print(decode_hello)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long) #long sequence of integers\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02885b-3553-4c7d-bbb1-eaed1d39880f",
   "metadata": {},
   "source": [
    "## Validation and training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133cd546-a9b6-46f9-b5dc-bb7b9139c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683e4420-a968-48d9-a3a0-c7dc8d4ad926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([178220, 321309, 165356, 361410])\n",
      "inputs:\n",
      "tensor([[ 1, 60, 61, 71,  1, 69, 73, 57],\n",
      "        [ 0, 90, 46, 60, 53, 72, 57, 74],\n",
      "        [56,  1, 72, 60, 53, 72,  1, 53],\n",
      "        [57, 66,  1, 60, 57,  1, 72, 73]], device='mps:0')\n",
      "targets:\n",
      "tensor([[60, 61, 71,  1, 69, 73, 57, 71],\n",
      "        [90, 46, 60, 53, 72, 57, 74, 57],\n",
      "        [ 1, 72, 60, 53, 72,  1, 53, 64],\n",
      "        [66,  1, 60, 57,  1, 72, 73, 70]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split =='train' else test_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,)) # takes a random integer between 1 and end of len(data),represent positions in the text where sequences will be extracted from.\n",
    "    print(ix) # random indices from the text to start generating from\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix]) # For each index in ix, it extracts a chunk of block_size characters (a sequence) from the data.\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # This is used to predict the next character in a sequence during training.\n",
    "    X,y = X.to(device), y.to(device)\n",
    "    return X,y\n",
    "\n",
    "X,y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(X.shape)\n",
    "print(X)\n",
    "print('targets:')\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a06bd42-3afc-4701-9b58-29de7000f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([43])  target is  tensor(61)\n",
      "when input is  tensor([43, 61])  target is  tensor(72)\n",
      "when input is  tensor([43, 61, 72])  target is  tensor(64)\n",
      "when input is  tensor([43, 61, 72, 64])  target is  tensor(57)\n",
      "when input is  tensor([43, 61, 72, 64, 57])  target is  tensor(21)\n",
      "when input is  tensor([43, 61, 72, 64, 57, 21])  target is  tensor(1)\n",
      "when input is  tensor([43, 61, 72, 64, 57, 21,  1])  target is  tensor(43)\n",
      "when input is  tensor([43, 61, 72, 64, 57, 21,  1, 43])  target is  tensor(60)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t] \n",
    "    print('when input is ', context, ' target is ', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80192f9-3ccd-47f2-8d77-3de22baa20ae",
   "metadata": {},
   "source": [
    "## Initializing the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5b31c3-533b-4fab-80a9-60bc7172950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G0jR6Xœ;ififA‘C*O!sX“z1Væ”.\n",
      " 2vj_æiS½:£C6à£?23Qd M3Gd#:leèOacN:zfAæm—!FèD)æéXœTYk26LJU]a3æ“k3F2YNSDI£*KZ½cc£g‘4½B[mDâ6gV‘bScCrAH—((L#1Ee(:âQ”*PifQe29âpDfGWG( lQ”“æi]æéNE.kifd2qOâ—(S[1vaFC7yâqI’e1—UP,q.½L—Ru9â?PZ[O!æ\n",
      "H8gse((‘CBèO16LhCENU.[TàesX;;rpxJ[‘tsX£?8YvWWW’Lf!q, eY3RRL11vG(rnBvyMà:Q9Vg,âZTjèG”\n",
      "K#uHZz”7WPe½vu.âqdKCaPU2qOa8FA?½£’fEt“9ccYJ;f?7tZ8?7pWrSC;X7wc”hEGVio3àFa)7WWW‘W“‘C:-”£—0e(RfNq.w8P,1;6œR6Zm\n",
      "gp\n",
      "n0\n",
      "W(£MPw£—zr’1;YéNE“Wâl*I g Ijàæi7W“G&”£‘G0W.UàMuv3\n",
      "èOqIXdVL—::[TZGNSw8½D;c”Qæâ?’W2q&O\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # giant grid for predictions, high probability of i coming after an r, \n",
    "        # normalize each row to predict what should come after each letter (it should have the highest probability).\n",
    "    \n",
    "    # Writing a forward pass function from scatch, best practice\n",
    "    def forward(self, index, targets = None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # Batch, time, and channels(vocab_size), unpacks them\n",
    "            logits = logits.view(B*T, C) # reshapes them since it requires (N,C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]# becomes (B,C)\n",
    "            # apply sofmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim = 1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device= device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec0879-1a0e-42ec-ba72-861d1224b727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
